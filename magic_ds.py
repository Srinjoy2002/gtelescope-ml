# -*- coding: utf-8 -*-
"""MAGIC_ds.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Srinjoy2002/gtelescope-ml/blob/master/MAGIC_ds.ipynb
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from imblearn.over_sampling import RandomOverSampler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow import keras
"""The data are MC generated (see below) to simulate registration of high energy gamma particles in a ground-based atmospheric Cherenkov gamma telescope using the imaging technique. Cherenkov gamma telescope observes high energy gamma rays, taking advantage of the radiation emitted by charged particles produced inside the electromagnetic showers initiated by the gammas, and developing in the atmosphere. This Cherenkov radiation (of visible to UV wavelengths) leaks through the atmosphere and gets recorded in the detector, allowing reconstruction of the shower parameters. The available information consists of pulses left by the incoming Cherenkov photons on the photomultiplier tubes, arranged in a plane, the camera. Depending on the energy of the primary gamma, a total of few hundreds to some 10000 Cherenkov photons get collected, in patterns (called the shower image), allowing to discriminate statistically those caused by primary gammas (signal) from the images of hadronic showers initiated by cosmic rays in the upper atmosphere (background).

"""

pd.read_csv("magic04.data")

cols=["fLength", "fWidth","fSize","fConc", "fConc1", "fAsym", "fM3Long","fM3Trans","fAlpha","fDist","class"]
df= pd.read_csv("magic04.data",names=cols)

# df.head()

"""converting g=1 and h=0 so that compi=uter can easily understand"""

df["class"].unique()

df["class"]=(df["class"]=="g").astype(int)

# df.head()

df["class"].unique()

"""Visualising the data in form of bar graphs

"""

# for label in cols[:-1]:
#   plt.hist(df[df["class"]==1][label], color='blue', label='gamma', alpha=0.6, density=True)
#   plt.hist(df[df["class"]==0][label], color='red', label='hadron', alpha=0.6, density=True)
#   plt.title(label)
#   plt.ylabel("probability")
#   plt.xlabel(label)
#   plt.legend()
#   plt.show()

"""Scaling the data"""

def scale_dataset(dataframe, oversample= False):
  x= dataframe[dataframe.columns[:-1]].values
  y= dataframe[dataframe.columns[-1]].values

  scaler=StandardScaler()
  x= scaler.fit_transform(x)

  if oversample:
    ros= RandomOverSampler()
    x,y = ros.fit_resample(x,y)

  data = np.hstack((x, np.reshape(y, (len(y), 1))))

  return data, x, y

train, valid, test = np.split(df.sample(frac=1), [int(0.6*len(df)), int(0.8*len(df))])

train, x_train, y_train = scale_dataset(train, oversample=True)
valid, x_valid, y_valid = scale_dataset(valid, oversample=False)
test, x_test, y_test = scale_dataset(test, oversample=False)

#we see there are lesser number of hadron so we need to oversample the hadron using oversampler import

"""KNN algorithm implementation"""

# knn_model=KNeighborsClassifier(n_neighbors=5)

# knn_model.fit(x_train,y_train)

# y_pred= knn_model.predict(x_test)

# print(classification_report(y_test, y_pred))

# #this is the best possible algorithm other than SVM

# #logistic regression implementation

# from sklearn.linear_model import LogisticRegression

# lg_model=LogisticRegression()
# lg_model=lg_model.fit(x_train,y_train)
# y_pred= lg_model.predict(x_test)

# print(classification_report(y_test,y_pred))

"""SUPPORT VECTOR MACHINES"""

from sklearn.svm import SVC

svm_model= SVC()

svm_model=svm_model.fit(x_train,y_train)

y_pred=svm_model.predict(x_test)
print(classification_report(y_test,y_pred))



# def plot_history(history):
#   fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))
#   ax1.plot(history.history['loss'], label='loss')
#   ax1.plot(history.history['val_loss'], label='val_loss')
#   ax1.set_xlabel('Epoch')
#   ax1.set_ylabel('Binary crossentropy')
#   ax1.grid(True)

#   ax2.plot(history.history['accuracy'], label='accuracy')
#   ax2.plot(history.history['val_accuracy'], label='val_accuracy')
#   ax2.set_xlabel('Epoch')
#   ax2.set_ylabel('Accuracy')
#   ax2.grid(True)

#   plt.show()

# def train_model(X_train, y_train, num_nodes, dropout_prob, lr, batch_size, epochs):
#   nn_model = tf.keras.Sequential([
#       tf.keras.layers.Dense(num_nodes, activation='relu', input_shape=(10,)),
#       tf.keras.layers.Dropout(dropout_prob),
#       tf.keras.layers.Dense(num_nodes, activation='relu'),
#       tf.keras.layers.Dropout(dropout_prob),
#       tf.keras.layers.Dense(1, activation='sigmoid')
#   ])

#   nn_model.compile(optimizer=tf.keras.optimizers.Adam(lr), loss='binary_crossentropy',
#                   metrics=['accuracy'])
#   history = nn_model.fit(
#     X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0
#   )

#   return nn_model, history

# least_val_loss = float('inf')
# least_loss_model = None
# epochs=100
# for num_nodes in [16, 32, 64]:
#   for dropout_prob in[0, 0.2]:
#     for lr in [0.01, 0.005, 0.001]:
#       for batch_size in [32, 64, 128]:
#         print(f"{num_nodes} nodes, dropout {dropout_prob}, lr {lr}, batch size {batch_size}")
#         model, history = train_model(x_train, y_train, num_nodes, dropout_prob, lr, batch_size, epochs)
#         plot_history(history)
#         val_loss = model.evaluate(x_valid, y_valid)[0]
#         if val_loss < least_val_loss:
#           least_val_loss = val_loss
#           least_loss_model = model

# y_pred = least_loss_model.predict(x_test)
# y_pred = (y_pred > 0.5).astype(int).reshape(-1,)

# print(classification_report(y_test, y_pred))

